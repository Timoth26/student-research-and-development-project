{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# directory path\n",
        "train = ''\n",
        "train_img_dir = ''\n",
        "test =''\n",
        "test_img_dir =''\n",
        "valid =''\n",
        "val_img_dir =''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wczytywanie obrazów i etykiet treningowych\n",
        "train_images = []\n",
        "train_labels = []\n",
        "for idx, row in train.iterrows():\n",
        "    img_path = os.path.join(train_img_dir, row['FILENAME'])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    train_images.append(img)\n",
        "    train_labels.append(row['IDENTITY'])\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Wczytywanie obrazów i etykiet testowych\n",
        "test_images = []\n",
        "test_labels = []\n",
        "for idx, row in test.iterrows():\n",
        "    img_path = os.path.join(test_img_dir, row['FILENAME'])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    test_images.append(img)\n",
        "    test_labels.append(row['IDENTITY'])\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "# Wczytywanie obrazów i etykiet validacyjnych\n",
        "val_images = []\n",
        "val_labels = []\n",
        "for idx, row in valid.iterrows():\n",
        "    img_path = os.path.join(val_img_dir, row['FILENAME'])\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    val_images.append(img)\n",
        "    val_labels.append(row['IDENTITY'])\n",
        "\n",
        "val_images = np.array(val_images)\n",
        "val_labels = np.array(val_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Załadowanie danych MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape danych\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalizacja danych\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Konwersja etykiet do postaci one-hot\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Podział danych na zbiór treningowy i walidacyjny\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train_images, train_labels = x_train, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Przykładowe funkcje do przetwarzania wstępnego obrazów\n",
        "def scale_images(images):\n",
        "    # Implementacja skalowania obrazów\n",
        "    scaled_images = images * 255  # Przykładowe skalowanie\n",
        "    return scaled_images\n",
        "\n",
        "def normalize_images(images):\n",
        "    # Implementacja normalizacji obrazów\n",
        "    normalized_images = images / 255  # Przykładowa normalizacja\n",
        "    return normalized_images\n",
        "\n",
        "def segment_images(images):\n",
        "    # Implementacja segmentacji obrazów\n",
        "    segmented_images = images  # Przykładowa segmentacja (brak zmiany)\n",
        "    return segmented_images\n",
        "\n",
        "# Funkcja do tworzenia modelu OCR\n",
        "def create_ocr_model(num_layers=2, filter_size=3, activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(filter_size, filter_size), activation=activation, input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Conv2D(32, kernel_size=(filter_size, filter_size), activation=activation))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Funkcja do przetwarzania wstępnego obrazów\n",
        "def preprocess_images(images, scaling=False, normalization=False, segmentation=False):\n",
        "    processed_images = images.copy()\n",
        "    \n",
        "    if scaling:\n",
        "        processed_images = scale_images(processed_images)\n",
        "    \n",
        "    if normalization:\n",
        "        processed_images = normalize_images(processed_images)\n",
        "    \n",
        "    if segmentation:\n",
        "        processed_images = segment_images(processed_images)\n",
        "    \n",
        "    return processed_images\n",
        "\n",
        "# Funkcja do oceny dokładności modelu OCR\n",
        "def evaluate_ocr_model(images, labels):\n",
        "    model = create_ocr_model()\n",
        "    model.fit(images, labels, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "    \n",
        "    # Ewaluacja modelu\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(labels, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "\n",
        "# Funkcja do przeprowadzenia eksperymentu z doborem hiperparametrów\n",
        "def run_hyperparameter_experiment(num_layers, filter_size, activation):\n",
        "    model = create_ocr_model(num_layers=num_layers, filter_size=filter_size, activation=activation)\n",
        "\n",
        "    # Trening modelu\n",
        "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val), verbose=0)\n",
        "\n",
        "    # Ewaluacja modelu\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "\n",
        "# Eksperymenty z przetwarzaniem wstępnym i doborem hiperparametrów\n",
        "def experiment():\n",
        "    \n",
        "    # Eksperyment z przetwarzaniem wstępnym\n",
        "    def preprocess_experiment():\n",
        "        results = []\n",
        "\n",
        "        # Eksperyment 1: Przetwarzanie wstępne obrazów - skalowanie i normalizacja\n",
        "        processed_images = preprocess_images(train_images, scaling=True, normalization=True)\n",
        "        accuracy_1, report_1 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Scaling + Normalization', 'accuracy': accuracy_1, 'report': report_1})\n",
        "\n",
        "        # Eksperyment 2: Przetwarzanie wstępne obrazów - segmentacja\n",
        "        processed_images = preprocess_images(train_images, segmentation=True)\n",
        "        accuracy_2, report_2 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Segmentation', 'accuracy': accuracy_2, 'report': report_2})\n",
        "\n",
        "        # Eksperyment 3: Przetwarzanie wstępne obrazów - skalowanie, normalizacja i segmentacja\n",
        "        processed_images = preprocess_images(train_images, scaling=True, normalization=True, segmentation=True)\n",
        "        accuracy_3, report_3 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Scaling + Normalization + Segmentation', 'accuracy': accuracy_3, 'report': report_3})\n",
        "\n",
        "        # Wyświetlenie wyników eksperymentów przetwarzania wstępnego\n",
        "        for result in results:\n",
        "            print(f\"Preprocessing Techniques: {result['preprocessing']}\")\n",
        "            print(f\"Accuracy: {result['accuracy']}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(result['report'])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "    # Eksperyment z doborem hiperparametrów\n",
        "    def hyperparameter_experiment():\n",
        "        hyperparameter_results = []\n",
        "\n",
        "        # Eksperyment 1: Defaultowe hiperparametry\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=3, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Default', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 2: Różna liczba warstw\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=3, filter_size=3, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': '3 Layers', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 3: Różny rozmiar filtra\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=5, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Filter Size 5x5', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 4: Różna funkcja aktywacji\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=3, activation='tanh')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Tanh Activation', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Wyświetlenie wyników eksperymentów z doborem hiperparametrów\n",
        "        for result in hyperparameter_results:\n",
        "            print(f\"Hyperparameters: {result['hyperparameters']}\")\n",
        "            print(f\"Accuracy: {result['accuracy']}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(result['report'])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    preprocess_experiment()\n",
        "    hyperparameter_experiment()\n",
        "\n",
        "experiment()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
