{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ścieżki do katalogów z danymi\n",
        "train_img_dir = '/handwriting-recognition/train_v2/train/'\n",
        "val_img_dir = '/handwriting-recognition/validation_v2/validation/'\n",
        "test_img_dir = '/handwriting-recognition/test_v2/test/'\n",
        "train_csv = '/handwriting-recognition/written_name_train_v2.csv'\n",
        "validation_csv = '/handwriting-recognition/written_name_validation_v2.csv'\n",
        "test_csv = '/handwriting-recognition/written_name_test_v2.csv'\n",
        "\n",
        "train = pd.read_csv(train_csv)\n",
        "valid = pd.read_csv(validation_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "\n",
        "train.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Reshape, BatchNormalization, Bidirectional, LSTM, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "train = train.sample(frac=0.01, random_state=42)\n",
        "valid = valid.sample(frac=0.01, random_state=42)\n",
        "\n",
        "\n",
        "# Clean data\n",
        "train.dropna(axis=0, inplace=True)\n",
        "valid.dropna(axis=0, inplace=True)\n",
        "\n",
        "# Remove unreadable labels\n",
        "train = train[train['IDENTITY'] != 'UNREADABLE']\n",
        "valid = valid[valid['IDENTITY'] != 'UNREADABLE']\n",
        "\n",
        "# Convert labels to uppercase\n",
        "train['IDENTITY'] = train['IDENTITY'].str.upper()\n",
        "valid['IDENTITY'] = valid['IDENTITY'].str.upper()\n",
        "\n",
        "# Reset indices\n",
        "train.reset_index(inplace=True, drop=True)\n",
        "valid.reset_index(inplace=True, drop=True)\n",
        "\n",
        "train_size = len(train)\n",
        "valid_size = len(valid)\n",
        "# Define different preprocessing techniques\n",
        "def preprocess_v1(img):\n",
        "    (h, w) = img.shape\n",
        "    final_img = np.ones([64, 256]) * 255  # blank white image\n",
        "    if w > 256:\n",
        "        img = img[:, :256]\n",
        "    if h > 64:\n",
        "        img = img[:64, :]\n",
        "    final_img[:h, :w] = img\n",
        "    return cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "def preprocess_v2(img):\n",
        "    img = cv2.resize(img, (256, 64))\n",
        "    return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "preprocess_methods = [preprocess_v1, preprocess_v2]\n",
        "\n",
        "# Define different sets of hyperparameters\n",
        "hyperparams = [\n",
        "    {'conv_filters': [32, 64, 128], 'lstm_units': [128, 64]},\n",
        "    {'conv_filters': [64, 128, 256], 'lstm_units': [256, 128]},\n",
        "]\n",
        "\n",
        "# Iterate through different preprocessing methods\n",
        "for preprocess_method in preprocess_methods:\n",
        "    # Load and preprocess the data\n",
        "    train_x = []\n",
        "    for i in range(train_size):\n",
        "        img_dir = '/content/handwriting-recognition/train_v2/train/' + train.loc[i, 'FILENAME']\n",
        "        image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
        "        image = preprocess_method(image)\n",
        "        image = image / 255.\n",
        "        train_x.append(image)\n",
        "\n",
        "    valid_x = []\n",
        "    for i in range(valid_size):\n",
        "        img_dir = '/content/handwriting-recognition/validation_v2/validation/' + valid.loc[i, 'FILENAME']\n",
        "        image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n",
        "        image = preprocess_method(image)\n",
        "        image = image / 255.\n",
        "        valid_x.append(image)\n",
        "\n",
        "    train_x = np.array(train_x).reshape(-1, 256, 64, 1)\n",
        "    valid_x = np.array(valid_x).reshape(-1, 256, 64, 1)\n",
        "\n",
        "    # Prepare labels and other data\n",
        "    def label_to_num(label):\n",
        "        label_num = []\n",
        "        for ch in label:\n",
        "            label_num.append(alphabets.find(ch))\n",
        "        return np.array(label_num)\n",
        "\n",
        "    train_y = np.ones([train_size, max_str_len]) * -1\n",
        "    train_label_len = np.zeros([train_size, 1])\n",
        "    train_input_len = np.ones([train_size, 1]) * (num_of_timestamps-2)\n",
        "    train_output = np.zeros([train_size])\n",
        "\n",
        "    for i in range(train_size):\n",
        "        train_label_len[i] = len(train.loc[i, 'IDENTITY'])\n",
        "        train_y[i, 0:len(train.loc[i, 'IDENTITY'])] = label_to_num(train.loc[i, 'IDENTITY'])\n",
        "\n",
        "    valid_y = np.ones([valid_size, max_str_len]) * -1\n",
        "    valid_label_len = np.zeros([valid_size, 1])\n",
        "    valid_input_len = np.ones([valid_size, 1]) * (num_of_timestamps-2)\n",
        "    valid_output = np.zeros([valid_size])\n",
        "\n",
        "    for i in range(valid_size):\n",
        "        valid_label_len[i] = len(valid.loc[i, 'IDENTITY'])\n",
        "        valid_y[i, 0:len(valid.loc[i, 'IDENTITY'])] = label_to_num(valid.loc[i, 'IDENTITY'])\n",
        "\n",
        "    # Iterate through different sets of hyperparameters\n",
        "    for params in hyperparams:\n",
        "        # Define the model using the Functional API\n",
        "        input_data = Input(shape=(256, 64, 1), name='input')\n",
        "        inner = Conv2D(params['conv_filters'][0], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(input_data)\n",
        "        inner = MaxPooling2D(pool_size=(2, 2))(inner)\n",
        "        inner = Conv2D(params['conv_filters'][1], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inner)\n",
        "        inner = MaxPooling2D(pool_size=(2, 2))(inner)\n",
        "        inner = Conv2D(params['conv_filters'][2], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inner)\n",
        "        inner = MaxPooling2D(pool_size=(2, 2))(inner)\n",
        "\n",
        "        # Calculate the correct reshape dimensions\n",
        "        inner_shape = inner.shape\n",
        "        new_shape = (inner_shape[1] * inner_shape[2], inner_shape[3])\n",
        "        inner = Reshape(target_shape=new_shape, name='reshape')(inner)\n",
        "\n",
        "        inner = Dense(64, activation='relu', kernel_initializer='he_normal', name='dense1')(inner)\n",
        "        inner = Bidirectional(LSTM(params['lstm_units'][0], return_sequences=True, dropout=0.25))(inner)\n",
        "        inner = Bidirectional(LSTM(params['lstm_units'][1], return_sequences=True, dropout=0.25))(inner)\n",
        "\n",
        "        inner = Dense(num_of_characters, activation='softmax', kernel_initializer='he_normal', name='dense2')(inner)\n",
        "\n",
        "        # CTC Loss function\n",
        "        def ctc_lambda_func(args):\n",
        "            y_pred, labels, input_length, label_length = args\n",
        "            return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        "\n",
        "        labels = Input(name='the_labels', shape=[max_str_len], dtype='float32')\n",
        "        input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "        label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        "\n",
        "        ctc_loss = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([inner, labels, input_length, label_length])\n",
        "\n",
        "        model = Model(inputs=[input_data, labels, input_length, label_length], outputs=ctc_loss)\n",
        "\n",
        "        model.compile(optimizer='adam', loss={'ctc': lambda y_true, y_pred: y_pred})\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        # Prepare inputs for the model\n",
        "        train_input = {\n",
        "            'input': train_x,\n",
        "            'the_labels': train_y,\n",
        "            'input_length': train_input_len,\n",
        "            'label_length': train_label_len\n",
        "        }\n",
        "\n",
        "        valid_input = {\n",
        "            'input': valid_x,\n",
        "            'the_labels': valid_y,\n",
        "            'input_length': valid_input_len,\n",
        "            'label_length': valid_label_len\n",
        "        }\n",
        "\n",
        "        # Dummy output, we will not use this\n",
        "        train_output = np.zeros([train_size])\n",
        "        valid_output = np.zeros([valid_size])\n",
        "\n",
        "        # Train the model\n",
        "        history = model.fit(train_input, train_output,\n",
        "                            validation_data=(valid_input, valid_output),\n",
        "                            epochs=10, batch_size=128)\n",
        "\n",
        "        # Log the results\n",
        "        print(f\"Results for preprocessing method {preprocess_method.__name__} and hyperparameters {params}:\")\n",
        "        print(history.history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funkcja do wczytywania i przetwarzania obrazów\n",
        "def load_images_and_labels(df, img_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for idx, row in df.iterrows():\n",
        "        img_path = os.path.join(img_dir, row['FILENAME'])\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Wczytaj obraz w skali szarości\n",
        "        img = cv2.resize(img, (28, 28))  # Zmiana rozmiaru na 28x28\n",
        "        images.append(img)\n",
        "        labels.append(row['IDENTITY'])\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n",
        "\n",
        "# Wczytywanie i przetwarzanie obrazów\n",
        "train_images, train_labels = load_images_and_labels(train, train_img_dir)\n",
        "val_images, val_labels = load_images_and_labels(valid, val_img_dir)\n",
        "test_images, test_labels = load_images_and_labels(test, test_img_dir)\n",
        "\n",
        "# Reshape danych\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "val_images = val_images.reshape(val_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalizacja danych\n",
        "train_images = train_images.astype('float32') / 255\n",
        "val_images = val_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Konwersja etykiet do postaci one-hot (zakładając, że mamy funkcję do mapowania nazw na indeksy)\n",
        "def labels_to_indices(labels, unique_labels):\n",
        "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    return np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "unique_labels = np.unique(np.concatenate([train_labels, val_labels, test_labels]))\n",
        "train_labels_indices = labels_to_indices(train_labels, unique_labels)\n",
        "val_labels_indices = labels_to_indices(val_labels, unique_labels)\n",
        "test_labels_indices = labels_to_indices(test_labels, unique_labels)\n",
        "\n",
        "train_labels_one_hot = to_categorical(train_labels_indices, len(unique_labels))\n",
        "val_labels_one_hot = to_categorical(val_labels_indices, len(unique_labels))\n",
        "test_labels_one_hot = to_categorical(test_labels_indices, len(unique_labels))\n",
        "\n",
        "# Podział danych na zbiór treningowy i walidacyjny (jeśli jeszcze nie zostało to zrobione)\n",
        "# x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Przypisanie danych do zmiennych używanych w kodzie \n",
        "x_train, y_train = train_images, train_labels_one_hot\n",
        "x_val, y_val = val_images, val_labels_one_hot\n",
        "x_test, y_test = test_images, test_labels_one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Załadowanie danych MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape danych\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalizacja danych\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Konwersja etykiet do postaci one-hot\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Podział danych na zbiór treningowy i walidacyjny\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train_images, train_labels = x_train, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Przykładowe funkcje do przetwarzania wstępnego obrazów\n",
        "def scale_images(images):\n",
        "    # Implementacja skalowania obrazów\n",
        "    scaled_images = images * 255  # Przykładowe skalowanie\n",
        "    return scaled_images\n",
        "\n",
        "def normalize_images(images):\n",
        "    # Implementacja normalizacji obrazów\n",
        "    normalized_images = images / 255  # Przykładowa normalizacja\n",
        "    return normalized_images\n",
        "\n",
        "def segment_images(images):\n",
        "    # Implementacja segmentacji obrazów\n",
        "    segmented_images = images  # Przykładowa segmentacja (brak zmiany)\n",
        "    return segmented_images\n",
        "\n",
        "# Funkcja do tworzenia modelu OCR\n",
        "def create_ocr_model(num_layers=2, filter_size=3, activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(filter_size, filter_size), activation=activation, input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Conv2D(32, kernel_size=(filter_size, filter_size), activation=activation))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Funkcja do przetwarzania wstępnego obrazów\n",
        "def preprocess_images(images, scaling=False, normalization=False, segmentation=False):\n",
        "    processed_images = images.copy()\n",
        "    \n",
        "    if scaling:\n",
        "        processed_images = scale_images(processed_images)\n",
        "    \n",
        "    if normalization:\n",
        "        processed_images = normalize_images(processed_images)\n",
        "    \n",
        "    if segmentation:\n",
        "        processed_images = segment_images(processed_images)\n",
        "    \n",
        "    return processed_images\n",
        "\n",
        "# Funkcja do oceny dokładności modelu OCR\n",
        "def evaluate_ocr_model(images, labels):\n",
        "    model = create_ocr_model()\n",
        "    model.fit(images, labels, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "    \n",
        "    # Ewaluacja modelu\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(labels, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "\n",
        "# Funkcja do przeprowadzenia eksperymentu z doborem hiperparametrów\n",
        "def run_hyperparameter_experiment(num_layers, filter_size, activation):\n",
        "    model = create_ocr_model(num_layers=num_layers, filter_size=filter_size, activation=activation)\n",
        "\n",
        "    # Trening modelu\n",
        "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val), verbose=0)\n",
        "\n",
        "    # Ewaluacja modelu\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "\n",
        "# Eksperymenty z przetwarzaniem wstępnym i doborem hiperparametrów\n",
        "def experiment():\n",
        "    \n",
        "    # Eksperyment z przetwarzaniem wstępnym\n",
        "    def preprocess_experiment():\n",
        "        results = []\n",
        "\n",
        "        # Eksperyment 1: Przetwarzanie wstępne obrazów - skalowanie i normalizacja\n",
        "        processed_images = preprocess_images(train_images, scaling=True, normalization=True)\n",
        "        accuracy_1, report_1 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Scaling + Normalization', 'accuracy': accuracy_1, 'report': report_1})\n",
        "\n",
        "        # Eksperyment 2: Przetwarzanie wstępne obrazów - segmentacja\n",
        "        processed_images = preprocess_images(train_images, segmentation=True)\n",
        "        accuracy_2, report_2 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Segmentation', 'accuracy': accuracy_2, 'report': report_2})\n",
        "\n",
        "        # Eksperyment 3: Przetwarzanie wstępne obrazów - skalowanie, normalizacja i segmentacja\n",
        "        processed_images = preprocess_images(train_images, scaling=True, normalization=True, segmentation=True)\n",
        "        accuracy_3, report_3 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Scaling + Normalization + Segmentation', 'accuracy': accuracy_3, 'report': report_3})\n",
        "\n",
        "        # Wyświetlenie wyników eksperymentów przetwarzania wstępnego\n",
        "        for result in results:\n",
        "            print(f\"Preprocessing Techniques: {result['preprocessing']}\")\n",
        "            print(f\"Accuracy: {result['accuracy']}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(result['report'])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "    # Eksperyment z doborem hiperparametrów\n",
        "    def hyperparameter_experiment():\n",
        "        hyperparameter_results = []\n",
        "\n",
        "        # Eksperyment 1: Defaultowe hiperparametry\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=3, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Default', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 2: Różna liczba warstw\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=3, filter_size=3, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': '3 Layers', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 3: Różny rozmiar filtra\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=5, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Filter Size 5x5', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 4: Różna funkcja aktywacji\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=3, activation='tanh')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Tanh Activation', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Wyświetlenie wyników eksperymentów z doborem hiperparametrów\n",
        "        for result in hyperparameter_results:\n",
        "            print(f\"Hyperparameters: {result['hyperparameters']}\")\n",
        "            print(f\"Accuracy: {result['accuracy']}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(result['report'])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    preprocess_experiment()\n",
        "    hyperparameter_experiment()\n",
        "\n",
        "experiment()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
