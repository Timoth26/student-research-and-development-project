{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import cv2\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ścieżki do katalogów z danymi\n",
        "train_img_dir = '/handwriting-recognition/train_v2/train/'\n",
        "val_img_dir = '/handwriting-recognition/validation_v2/validation/'\n",
        "test_img_dir = '/handwriting-recognition/test_v2/test/'\n",
        "train_csv = '/handwriting-recognition/written_name_train_v2.csv'\n",
        "validation_csv = '/handwriting-recognition/written_name_validation_v2.csv'\n",
        "test_csv = '/handwriting-recognition/written_name_test_v2.csv'\n",
        "\n",
        "train = pd.read_csv(train_csv)\n",
        "valid = pd.read_csv(validation_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "\n",
        "train.head(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Funkcja do wczytywania i przetwarzania obrazów\n",
        "def load_images_and_labels(df, img_dir):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for idx, row in df.iterrows():\n",
        "        img_path = os.path.join(img_dir, row['FILENAME'])\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Wczytaj obraz w skali szarości\n",
        "        img = cv2.resize(img, (28, 28))  # Zmiana rozmiaru na 28x28\n",
        "        images.append(img)\n",
        "        labels.append(row['IDENTITY'])\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    return images, labels\n",
        "\n",
        "# Wczytywanie i przetwarzanie obrazów\n",
        "train_images, train_labels = load_images_and_labels(train, train_img_dir)\n",
        "val_images, val_labels = load_images_and_labels(valid, val_img_dir)\n",
        "test_images, test_labels = load_images_and_labels(test, test_img_dir)\n",
        "\n",
        "# Reshape danych\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
        "val_images = val_images.reshape(val_images.shape[0], 28, 28, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalizacja danych\n",
        "train_images = train_images.astype('float32') / 255\n",
        "val_images = val_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# Konwersja etykiet do postaci one-hot (zakładając, że mamy funkcję do mapowania nazw na indeksy)\n",
        "def labels_to_indices(labels, unique_labels):\n",
        "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "    return np.array([label_to_index[label] for label in labels])\n",
        "\n",
        "unique_labels = np.unique(np.concatenate([train_labels, val_labels, test_labels]))\n",
        "train_labels_indices = labels_to_indices(train_labels, unique_labels)\n",
        "val_labels_indices = labels_to_indices(val_labels, unique_labels)\n",
        "test_labels_indices = labels_to_indices(test_labels, unique_labels)\n",
        "\n",
        "train_labels_one_hot = to_categorical(train_labels_indices, len(unique_labels))\n",
        "val_labels_one_hot = to_categorical(val_labels_indices, len(unique_labels))\n",
        "test_labels_one_hot = to_categorical(test_labels_indices, len(unique_labels))\n",
        "\n",
        "# Podział danych na zbiór treningowy i walidacyjny (jeśli jeszcze nie zostało to zrobione)\n",
        "# x_train, x_val, y_train, y_val = train_test_split(train_images, train_labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Przypisanie danych do zmiennych używanych w kodzie \n",
        "x_train, y_train = train_images, train_labels_one_hot\n",
        "x_val, y_val = val_images, val_labels_one_hot\n",
        "x_test, y_test = test_images, test_labels_one_hot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mnist dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Załadowanie danych MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape danych\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Normalizacja danych\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Konwersja etykiet do postaci one-hot\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Podział danych na zbiór treningowy i walidacyjny\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "train_images, train_labels = x_train, y_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Przykładowe funkcje do przetwarzania wstępnego obrazów\n",
        "def scale_images(images):\n",
        "    # Implementacja skalowania obrazów\n",
        "    scaled_images = images * 255  # Przykładowe skalowanie\n",
        "    return scaled_images\n",
        "\n",
        "def normalize_images(images):\n",
        "    # Implementacja normalizacji obrazów\n",
        "    normalized_images = images / 255  # Przykładowa normalizacja\n",
        "    return normalized_images\n",
        "\n",
        "def segment_images(images):\n",
        "    # Implementacja segmentacji obrazów\n",
        "    segmented_images = images  # Przykładowa segmentacja (brak zmiany)\n",
        "    return segmented_images\n",
        "\n",
        "# Funkcja do tworzenia modelu OCR\n",
        "def create_ocr_model(num_layers=2, filter_size=3, activation='relu'):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=(filter_size, filter_size), activation=activation, input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    for _ in range(num_layers - 1):\n",
        "        model.add(Conv2D(32, kernel_size=(filter_size, filter_size), activation=activation))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Funkcja do przetwarzania wstępnego obrazów\n",
        "def preprocess_images(images, scaling=False, normalization=False, segmentation=False):\n",
        "    processed_images = images.copy()\n",
        "    \n",
        "    if scaling:\n",
        "        processed_images = scale_images(processed_images)\n",
        "    \n",
        "    if normalization:\n",
        "        processed_images = normalize_images(processed_images)\n",
        "    \n",
        "    if segmentation:\n",
        "        processed_images = segment_images(processed_images)\n",
        "    \n",
        "    return processed_images\n",
        "\n",
        "# Funkcja do oceny dokładności modelu OCR\n",
        "def evaluate_ocr_model(images, labels):\n",
        "    model = create_ocr_model()\n",
        "    model.fit(images, labels, epochs=10, batch_size=32, validation_split=0.2, verbose=0)\n",
        "    \n",
        "    # Ewaluacja modelu\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(labels, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "\n",
        "# Funkcja do przeprowadzenia eksperymentu z doborem hiperparametrów\n",
        "def run_hyperparameter_experiment(num_layers, filter_size, activation):\n",
        "    model = create_ocr_model(num_layers=num_layers, filter_size=filter_size, activation=activation)\n",
        "\n",
        "    # Trening modelu\n",
        "    history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val), verbose=0)\n",
        "\n",
        "    # Ewaluacja modelu\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "    report = classification_report(y_true_classes, y_pred_classes)\n",
        "\n",
        "    return accuracy, report\n",
        "\n",
        "\n",
        "# Eksperymenty z przetwarzaniem wstępnym i doborem hiperparametrów\n",
        "def experiment():\n",
        "    \n",
        "    # Eksperyment z przetwarzaniem wstępnym\n",
        "    def preprocess_experiment():\n",
        "        results = []\n",
        "\n",
        "        # Eksperyment 1: Przetwarzanie wstępne obrazów - skalowanie i normalizacja\n",
        "        processed_images = preprocess_images(train_images, scaling=True, normalization=True)\n",
        "        accuracy_1, report_1 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Scaling + Normalization', 'accuracy': accuracy_1, 'report': report_1})\n",
        "\n",
        "        # Eksperyment 2: Przetwarzanie wstępne obrazów - segmentacja\n",
        "        processed_images = preprocess_images(train_images, segmentation=True)\n",
        "        accuracy_2, report_2 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Segmentation', 'accuracy': accuracy_2, 'report': report_2})\n",
        "\n",
        "        # Eksperyment 3: Przetwarzanie wstępne obrazów - skalowanie, normalizacja i segmentacja\n",
        "        processed_images = preprocess_images(train_images, scaling=True, normalization=True, segmentation=True)\n",
        "        accuracy_3, report_3 = evaluate_ocr_model(processed_images, train_labels)\n",
        "        results.append({'preprocessing': 'Scaling + Normalization + Segmentation', 'accuracy': accuracy_3, 'report': report_3})\n",
        "\n",
        "        # Wyświetlenie wyników eksperymentów przetwarzania wstępnego\n",
        "        for result in results:\n",
        "            print(f\"Preprocessing Techniques: {result['preprocessing']}\")\n",
        "            print(f\"Accuracy: {result['accuracy']}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(result['report'])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "\n",
        "    # Eksperyment z doborem hiperparametrów\n",
        "    def hyperparameter_experiment():\n",
        "        hyperparameter_results = []\n",
        "\n",
        "        # Eksperyment 1: Defaultowe hiperparametry\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=3, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Default', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 2: Różna liczba warstw\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=3, filter_size=3, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': '3 Layers', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 3: Różny rozmiar filtra\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=5, activation='relu')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Filter Size 5x5', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Eksperyment 4: Różna funkcja aktywacji\n",
        "        accuracy, report = run_hyperparameter_experiment(num_layers=2, filter_size=3, activation='tanh')\n",
        "        hyperparameter_results.append({'hyperparameters': 'Tanh Activation', 'accuracy': accuracy, 'report': report})\n",
        "\n",
        "        # Wyświetlenie wyników eksperymentów z doborem hiperparametrów\n",
        "        for result in hyperparameter_results:\n",
        "            print(f\"Hyperparameters: {result['hyperparameters']}\")\n",
        "            print(f\"Accuracy: {result['accuracy']}\")\n",
        "            print(\"Classification Report:\")\n",
        "            print(result['report'])\n",
        "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    preprocess_experiment()\n",
        "    hyperparameter_experiment()\n",
        "\n",
        "experiment()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
